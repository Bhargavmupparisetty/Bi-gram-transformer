# Bigram Transformer Model on Shakespeare Dataset

## Overview
This project implements a Bigram Transformer model trained on the works of William Shakespeare. The model is inspired by Andrej Karpathy's video tutorial on transformers and demonstrates the use of bigram tokenization to generate text in Shakespearean style.

## Features
- **Bigram Tokenization**: Splits text into bigram tokens for training.
- **Transformer Architecture**: Implements a simplified transformer model with self-attention and feed-forward layers.
- **Text Generation**: Generates new text sequences in the style of Shakespeare.

## Tools and Technologies
- **Python**
- **PyTorch**: For implementing the transformer architecture
- **Datasets**: Shakespeare's works
- **Numpy/Pandas**: For data preprocessing

## Installation
1. Clone this repository:
   ```bash
   git clone https://github.com/Bhargavmupparisetty/Bi-gram-transformer.git
   ```
2. Navigate to the project directory:
   ```bash
   cd Bi-gram-transformer
   ```

## Results
- The model successfully generates coherent Shakespeare-style text.
- Demonstrates the capability of bigram-based tokenization for language modeling tasks.

## Future Enhancements
- Upgrade to n-gram tokenization for richer context.
- Implement positional encodings for improved sequence learning.
- Experiment with larger transformer models for better performance.

## Acknowledgments
- This project is heavily inspired by Andrej Karpathy's video tutorial on transformers.
- The link to the tutorial
  ```bash
  https://youtu.be/kCc8FmEb1nY?si=W8S_GrgdUIZUxF91
  ```

